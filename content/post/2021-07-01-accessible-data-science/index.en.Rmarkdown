---
title: Accessible Data Science
author: Manohar Mulchandani
date: '2021-07-01'
slug: accessible-data-science
categories:
  - R Python
tags:
  - Data Science
  - Accessible
  - Lean
subtitle: 'Executing a Data Science project with Open Source tools'
summary: 'This is a story of how an impactful data science project was successfully executed using a very small team and by using open source tools.'
authors: []
lastmod: date
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---
<style>
body {
  background-color: cornsilk;
}
</style>

## Introduction
This is the story of a relatively small but impactful data science project that was executed with a very small team and by using popular Open Source data science tools that are accessible for everyone.

We begin with brief details of the problem being solved and the team that executed the project.

We follow that up with details of the Open Source toolset that we used and how and where we employed them.

Finally, we talk about some of the operational aspects of the project, that, in our view were critical to the success of the project.

## The Project
The goal of data science project was to estimate sales of new restaurants for a reputed restaurant chain. The existing approach used an Excel based tool based on business rules. The objective was to explore modern data science tools and methods to see if those could help improve the estimation.

### The Team
The project team comprised a Domain Expert and a Data Science consultant. There couldn't be a smaller team.

The Domain Expert was also the owner of the existing Excel based tool, was familiar with the data and brought the required business knowledge to the project.

The Data Science consultant had knowledge of the tools required for the project and the technical expertise to execute the project.

The project was overseen by a executive sponsor. We often leveraged senior managers who were knowledgeable about the business.

### The Data Science Toolset

> We relied on free, popular, widely used open source tools for executing this project.

#### Developer Tools
Our early work used **Python** and **Jupyter Notebooks**:
- Python scripts to move ETL logic out of Excel formulas.
- Jupyter notebooks for Exploratory Data Analysis (EDA)  with `Pandas` and `Matplotlib` being our primary work horses.

Later, we switched to **R** and [**Shiny**](https://shiny.rstudio.com/), with [**RStudio Desktop**](https://rstudio.com/products/rstudio/download/) being our IDE of choice. Using the `shiny` package, we built several Web based tools for doing the analysis. We did not need an additional Web Developer to create these Web based tools, since the `shiny` package makes it possible to create Web based tools using R. More on this later.

When we needed to access Python based ETL scripts from R, we used the R `reticulate` package to invoke the Python based ETL scripts from R. That saved us a rewrite of the ETL scripts, which were rather complicated. We were not wedded to Python or R: both are free and popular and good. We used them to their strengths.

We also made extensive use of the R [**tidyverse**](https://www.tidyverse.org) packages, which provide tools for working with data, starting from extracting data, transforming data and visualizing data. In case you are not familiar with **tidyverse**, it is like having `Pandas` and `Matplotlib` working in an integrated fashion. In addition to `ggplot2` from the tidyverse, we used `leaflet` packages for geo-spatial visualizations.

#### MLOps
For MLOps, we used [MLflow](www.mlflow.org): we  used the [MLFlow R API](https://www.mlflow.org/docs/latest/R-api.html) primarily for tracking the several models we built and for configuring the models to be deployed to production. The tracking data generated by MLFlow was used for configuring the best model for deployment.

All the Web Apps were deployed using the open source [**Shiny Server**](https://www.rstudio.com/products/shiny/shiny-server/) running on an RHEL Virtual Machine.

#### Databases
We had a choice to use the corporate SQL Server database or an SQLite database: we decided to use SQLite, since it gave us much more control and flexibility. SQLite worked for us because the team that used the models in production was small and it did not cause any concurrency issue that can crop up when using SQLite.

## Executing the Project
We focus on three things that we feel made a difference.

### Create Project Tools
We found that an integrated tool to perform and/or automate the various steps of the data science project lifecycle makes a difference. While Jupyter Notebooks or R Markdown Notebooks are great to get started with quickly, and some platforms do provide collaboration features, they can't beat an integrated tool.

> In our case, Jupyter and R Markdown notebooks were not accessible to the Domain Expert. Given the small size of the team, it was important that the project load was shared.

After the initial exploratory start with Jupyter notebooks, we soon realized that we needed Web based tools with point and click interface, so that the Domain Expert could participate in the analysis. He had the business knowledge and a good understanding of the data. 

> This is what prompted the switch to R. The `shiny` package allowed us to create a Web based tool, indeed several tools, without requiring knowledge of Web programming.

Overtime, we augmented the Web App to enable the entire life cycle: Exploratory Data Analysis, Building & Tuning Models, and Configuring & Deploying models for production use. We even slapped a separate UI onto the ETL process, to give us visibility into the ETL steps. As we enhanced the tool, it enabled the Domain Expert to independently execute all steps in a data science project lifecycle. 

An integrated Web based tool completely changed the dynamics of project execution. The Domain Expert and Data Science consultant could work independently, knowing that they were working with the same data, same visualizations and could examine the models each one was building, without having to always coordinate calendars.

Once we started building the models, we needed to iterate quickly over several models and not loose momentum. We needed the ability to do What-If analysis with different datasets and have the ability to add and experiment with new features. The integrated tool provided us with the harness to do our rapid experimentation. It would have been difficult to do so with a Jupyter or R Markdown Notebook, and would have slowed us down.

Very soon, it became a runaway activity with hundreds of models. Thanks to MLFlow that we integrated with the tool, we were able to track and manage the models that we created, obviating the need to write custom code for managing models.

> In hindsight, the Web based tools saved the day for us. 

We would argue that Web based integrated tools (or project specific integrated tools) are necessary even in a team of data scientists: it provides a common platform for the team, ensuring that all team members are working in a co-ordinated fashion. It streamlines the execution of the project. An integrated tool also enables participation by non-technical folks in the data science project.

### Visualizations

> Visualizations form the backbone of effective data analysis.

All our review meetings started with Visualizations. We used visualizations to share our findings.

The early visualizations that we created using `Matplotlib` showed promising patterns, patterns that instilled confidence in the business team. We couldn't have done that with data summaries.

All our Exploratory analysis was done using Visualizations. We used them to find patterns in data and to identify features that we could use for our models. Visualizations help us identify outliers. And to analyze and understand outliers, we used additional visualizations.

Occasionally we dipped into the data, and that too was informed by visualizations; we did so when we found anomalies in our visualizations.

All the visualizations were incorporated in the integrated tool that we built using the `shiny` package.

We did make use of data summaries, once in a while. However, summaries tend to hide detail, while visualizations expose the details.

### Know your data

> The importance of getting to know your data cannot be emphasized enough.

Ultimately, what really helped us was our knowledge of the data, knowledge that we built over time. It was only by understanding the data that we found our features or recognized outliers. This is also what helped us identify errors in data, so that we could fix them. The integrated tool made it easy for us to explore and understand the data, primarily through the various visualizations we created. 

It is often said that around 80% of the work in a data science project involves working with data. The percentages may wary depending on the quality of data.

Messy data is a reality. The systems that generate the data are built to make the user's job easy and are not designed to generate perfect data. In our case, we had to deal with accented characters, differences in data encoding between different systems for the same data, to name a few. These were mostly sorted out during the ETL phase. They occasionally crept up and were a nuisance.

However, most of the 80% really goes towards understanding the data. It is tedious, but it is important. Often, it feels like looking for a needle in a haystack: sometimes, it takes a while to realize that you are looking at the wrong haystack.

Visualizations do help reduce the tedium.

Geting to know your data takes time; indeed, it is the project.

# Closing Notes
The availability of a large number of high quality open source tools for data science enabled us to execute a high impact project of moderate complexity with low budget. We were able to keep costs further down by having a small team.

What I hope to convey by this story is that Data Science is indeed accessible to everyone, that impactful data science projects can be executed by small teams and with small budgets. The learnings from this story apply equally for big and small organisations: after all, even bigger organisations have a budget.

After reading this, I hope more teams feel enthused in taking up Data Science projects.

